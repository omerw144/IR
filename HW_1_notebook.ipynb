{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Information Retrieval\n",
    "## Instructions\n",
    "1. Students will form teams of three people each and submit a single homework for each team in the format - ID1_ID2_ID3.ipynb\n",
    "2. Groups of four are not allowed and you are more than welcome to form groups of two.\n",
    "2. **Do not write your names anywhere.**\n",
    "3. For the code part: \n",
    "> **Write your code only in the mentioned sections. Do not change the code of other sections**. Do not use any imports unless we say so.\n",
    "4. For theoretical questions, if any - write your answer in the markdown cell dedicated to this task, in **English**.\n",
    "\n",
    "\n",
    "#### Deviation from the aforementioned  instructions will lead to reduced grade\n",
    "---\n",
    "\n",
    "\n",
    "## Clarifications\n",
    "1. The same score for the homework will be given to each member of the team.  \n",
    "2. The goal of this homework is to test your understanding of the concepts presented in the lectures. \\\n",
    "If a topic was not covered in detail during the lecture, you are asked to study it online on your own. \n",
    "Anyhow, we provide here detailed explanations for the code part and if you have problems - ask.\n",
    "3. Questions can be sent to the forum, you are encouraged to ask questions but do so after you have been thinking about your question. \n",
    "4. The length of the empty gaps (where you are supposed to write your code) is a recommendation (the amount of space took us to write the solution) and writing longer code will not harm your grade. We do not expect you to use the programming tricks and hacks we used to make the code shorter.   \n",
    "Having said that, we do encourage you to write good code and keep that in mind - **extreme** cases may be downgraded.  \n",
    "We also encourage to use informative variable names - it is easier for us to check and for you to understand. \n",
    "\n",
    "For your convenience, , the code has a **DEBUG** mode that you may use in order to debug with toy data.  \n",
    "It is recommended to solve the code in that mode (with efficiency in mind) and then run the code on all the data.\n",
    "**Do not forget to file the HW with DEBUG == False**.\n",
    "\n",
    "`Download the data` from [HERE](https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics) and put it in the same directory your script is.\n",
    "\n",
    "Since it is the first time we provide this homework please notify us if there is a bug/something is unclear, typo's exc..\n",
    "\n",
    "5. We use Python 3.7 for programming.\n",
    "6. Make sure you have all the packages and functions used in the import section. Most of it is native to Anaconda Python distribution.\n",
    "\n",
    "### Have fun !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omerw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omerw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n",
    "\"\"\" you can change this cell \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\"\"\"\n",
    "Recommended to start with a small number to get a feeling for the preprocessing with prints (N_ROWS_FOR_DEBUG = 2)\n",
    "later increase this number for 5*10**3 in order to see that the code runs at reasonable speed. \n",
    "When setting Debug == False, our code implements bow.fit() in 15-20 minutes according to the tqdm progress bar. Your solution is not supposed to be much further than that.\n",
    "\"\"\"\n",
    "N_ROWS_FOR_DEBUG = 5*10**3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = Path(\"lyrics.csv\")\n",
    "BOW_PATH = Path(\"bow.csv\")\n",
    "N_ROWS = N_ROWS_FOR_DEBUG if DEBUG else None\n",
    "CHUNCK_SIZE = 5 if DEBUG else 5*10**3\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1 if DEBUG else 363*10**3//CHUNCK_SIZE + 1\n",
    "COLS = [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words /TfIdf model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update self.trigram count (three consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 73/73 [45:51<00:00, 37.69s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        ### YOUR CODE HERE\n",
    "        word = str(np.char.lower(word))\n",
    "        tmp_word = ''\n",
    "        for letter in word:\n",
    "            if letter in allowed_symbols:\n",
    "                tmp_word += letter\n",
    "        word = tmp_word\n",
    "        if (word in stop_words):\n",
    "            next\n",
    "        else:\n",
    "            word = stemmer.stem(word)\n",
    "            if len(word) > 1:\n",
    "                output_sentence.append(word)\n",
    "\n",
    "\n",
    "                          \n",
    "        ### END YOUR CODE\n",
    "    return output_sentence\n",
    "    \n",
    "\n",
    "def get_data_chuncks() -> List[str]:\n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))] \n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        self.unigram_count =  Counter()\n",
    "        self.bigram_count = Counter()\n",
    "        self.trigram_count = Counter()\n",
    "        self.document_term_frequency = Counter()\n",
    "        self.word_document_frequency = {}\n",
    "        self.inverted_index = {}\n",
    "        self.doc_norms = {}\n",
    "        self.n_docs = -1\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    def update_counts_and_probabilities(self, sentence :List[str],document_id:int) -> None:\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        for i,word in enumerate(sentence):\n",
    "            ### YOUR CODE HERE\n",
    "            self.unigram_count.update([word])\n",
    "            if i+1 < len(sentence):\n",
    "                self.bigram_count.update([word +' '+ sentence[i+1]])\n",
    "            if i + 2 < len(sentence):\n",
    "                self.trigram_count.update([word +' '+ sentence[i + 1]  +' '+ sentence[i + 2]])\n",
    "            if word not in self.inverted_index:\n",
    "                self.inverted_index[word] = {}\n",
    "            if document_id not in self.inverted_index[word]:\n",
    "                self.inverted_index[word][document_id] = 1\n",
    "            else:\n",
    "                self.inverted_index[word][document_id] += 1\n",
    "            \n",
    "\n",
    "                        \n",
    "            ### END YOUR CODE\n",
    "        \n",
    "        \n",
    "    def fit(self) -> None:\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):\n",
    "            for sentence in chunck: #sentence is a song (string)\n",
    "                self.n_docs += 1 \n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()\n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "             \n",
    "    def compute_word_document_frequency(self):\n",
    "        for word in self.inverted_index.keys():\n",
    "            ### YOUR CODE HERE\n",
    "             self.word_document_frequency[word] = len(self.inverted_index[word])\n",
    "\n",
    "            ### END YOUR CODE\n",
    "            \n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        ### YOUR CODE HERE\n",
    "        for word in self.inverted_index:\n",
    "            for document in self.inverted_index[word]:\n",
    "                self.inverted_index[word][document] /= float(self.word_document_frequency[word])\n",
    "                #norm\n",
    "                if document not in self.doc_norms:\n",
    "                    self.doc_norms[document] = self.inverted_index[word][document]**2\n",
    "                else:\n",
    "                    self.doc_norms[document] += self.inverted_index[word][document]**2\n",
    "        \n",
    "\n",
    "                \n",
    "        ### END YOUR CODE\n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) \n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)\n",
    "                \n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### YOUR SOLUTION HERE\n",
    "1. #k- the average of words in doc.\n",
    "#d- number of docs.\n",
    "tf complexity- o(k*d),\n",
    "df complexity- o(k*d)\n",
    "tf_idf= o(k*d) \n",
    "\n",
    "\n",
    "2.we can make this code better in terms running time  by using hash table.\n",
    "after doing hash table once, the complexity of deciding if a term is in a doc or not will become 0(1) instead of o(k*d).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the following methods:\n",
    "\n",
    "`reduce_query_to_counts`: given a list of words returns a counter object with words as keys and counts as values.\n",
    "\n",
    "`rank`: given a query and relevant documents calculate the similarity (cosine or inner product simialrity) between each document and the query.   \n",
    "Make sure to transform the query word counts to tf idf as well. \n",
    "\n",
    "`sort_and_retrieve_k_best`: returns the top k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        self.sentence_preprocesser = preprocess_sentence  \n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "        \n",
    "    def rank(self,query : Dict[str,int],documents: Dict[str,Counter],metric: str ) -> Dict[str, float]:\n",
    "        result = {} # key: DocID , value : float , simmilarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "        ### YOUR CODE HERE\n",
    "        for word in query:\n",
    "            #print(word)\n",
    "            query[word] /= query_len\n",
    "            #print('query[word] ',query[word])\n",
    "            for doc in documents[word]:\n",
    "                if doc not in result:\n",
    "                    result[doc] = query[word] * documents[word].get(doc)\n",
    "                else:\n",
    "                    result[doc] += query[word] * documents[word].get(doc)\n",
    "               \n",
    "         ### END YOUR CODE\n",
    "        if metric == 'cosine':\n",
    "            ### YOUR CODE HERE\n",
    "            for v in result:\n",
    "                #print (result[doc], self.doc_norms[doc])\n",
    "                result[v] /= (self.doc_norms[v])    #if we will divide in the query norm all the values the rank will be the same so no need\n",
    "\n",
    "        #print({key: result[key] for key in sorted(result, key=result.get, reverse=True)[:5]})\n",
    "\n",
    "        return result\n",
    "         ### END YOUR CODE\n",
    "    \n",
    "    def sort_and_retrieve_k_best(self, scores: Dict[str, float],k :int):\n",
    "        ### YOUR CODE HERE \n",
    "        return sorted(scores, key=scores.get, reverse=True)[:k]\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def reduce_query_to_counts(self, query: List) -> Counter:\n",
    "        ### YOUR CODE HERE\n",
    "        return Counter(query)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        \n",
    "    def get_top_k_documents(self,query : str, metric: str , k = 5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab] # filter nan \n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word : self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow,relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity,k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\omerw\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\IPython\\core\\display.py:694: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68983, 189048, 233333, 109203, 100158]\n",
      "[18869, 123673, 211934, 129254, 225923]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "Why\n",
      "Why don't you like me?\n",
      "Why\n",
      "Why don't you like me?\n",
      "Why\n",
      "Why don't you like me?\n",
      "Why\n",
      "Why don't you like me? \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "Feel me\n",
      "Feel me\n",
      "Feel me\n",
      "... \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "One, two, one, two, three, four\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "I feel like I do\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "All the years, I see\n",
      "(I see)\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "Let me know how you feel\n",
      "Let me know how you feel\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "I feel like I do\n",
      "(Like I do)\n",
      "All the years, I see\n",
      "(I see) \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should\n",
      "You should feel\n",
      "You should feel\n",
      "You should feel\n",
      "You should feel\n",
      "You should feel what I feel\n",
      "You should take what I take\n",
      "You should feel what I feel\n",
      "You should take what I take \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "How do you dream?\n",
      "How do you think?\n",
      "How do you do everything\n",
      "If you're not like me\n",
      "And you're just like me\n",
      "You are just like me.\n",
      "And what do you feel?\n",
      "What do you see?\n",
      "What do you see\n",
      "When it's dark you see you're like me\n",
      "You're just like me\n",
      "You're just like me.\n",
      "And where do you go?\n",
      "And where do you go?\n",
      "where do you go to get lost\n",
      "You know where I'll be\n",
      "'Cause you're just like me\n",
      "You're just like me \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [5]).iloc[cosine_top_k]['lyrics']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"tf_idf\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics size:  324632.382  KB\n",
      "bow size:  193319.838  KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n### Your verbal solution here\\nLyrics size:  324,632.382  KB\\nbow size:  193,319.838  KB\\n\\nthe lyrics file contains all the words of all the documents. the bow file contains just the unique words in each document (even that after removing some \\nillegal signs and stemming all the words) . \\nThe BOW method allows us to save our data in a numeric way instead of long strings\\nthe scores of a words in the documents don't cost us so much as the size of the original documents.\\n\\n\\n### End your verbal solution here\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "### YOUR SOLUTION HERE\n",
    "len(tf_idf.unigram_count)\n",
    "\n",
    "\n",
    "### END YOUR SOLUTION\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "We have 388243  unique words\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 2.\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "# if every pair of words where unique se we had\n",
    "sum(tf_idf.bigram_count.values())\n",
    "# because not all the pairs are unique, we have\n",
    "len(tf_idf.bigram_count)\n",
    "#which is\n",
    "len(tf_idf.bigram_count)\n",
    "### END YOUR SOLUTION\n",
    "#percent from the maximum value\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "if every pair of words where unique se we had 32,595,363 pairs.\n",
    "because not all the pairs are unique, we have 6,312,785 pairs,\n",
    "which is 19.3 percent from the maximum value\n",
    "the different is due to the  pairs of words in our corpus, which repeat themselves. \n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 3.\n",
    "### YOUR SOLUTION HERE\n",
    "print('Lyrics size: ',INPUT_FILE_PATH.stat().st_size / 1000.0, ' KB')\n",
    "print('bow size: ',BOW_PATH.stat().st_size / 1000.0, ' KB')\n",
    "### END YOUR SOLUTION\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "Lyrics size:  324,632.382  KB\n",
    "bow size:  193,319.838  KB\n",
    "\n",
    "the lyrics file contains all the words of all the documents. the bow file contains just the unique words in each document (even that after removing some \n",
    "illegal signs and stemming all the words) . \n",
    "The BOW method allows us to save our data in a numeric way instead of long strings\n",
    "the scores of a words in the documents don't cost us so much as the size of the original documents.\n",
    "\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (v) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$sim(v,w) := prior \\cdot likelihood = p(w) \\cdot P(v|w) $$ \n",
    "$$P(v|w) := JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing v to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor example - get_bigrams is a generator, which is an object we can loop on:\\nfor ngram in get_bigrams(word):\\n    DO SOMETHING\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "    \n",
    "def get_trigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 3):\n",
    "        yield \"\".join(list(ngram))\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "for example - get_bigrams is a generator, which is an object we can loop on:\n",
    "for ngram in get_bigrams(word):\n",
    "    DO SOMETHING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        self.unigram_counts = unigram_counts\n",
    "        self.ngram_index = {}\n",
    "        self.get_n_grams = get_n_gram\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "        ### YOUR CODE HERE\n",
    "          for word in self.unigram_counts:\n",
    "                self.ngram_index[word] = self.get_n_grams(word)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def get_top_k_words(self,word:str,k=5) -> List[str]:\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        all_unigrams_count = sum(self.unigram_counts.values())\n",
    "        word_ngram = list(self.get_n_grams(word))\n",
    "        #intersect\n",
    "        #for word_ng in word_ngram:\n",
    "        for ngram in self.ngram_index:\n",
    "            #print('ngram',ngram)\n",
    "            #print('word_ngram',word_ngram)\n",
    "            p_w = self.unigram_counts[ngram]/all_unigrams_count\n",
    "            self.ngram_index[ngram] = p_w * round(len((set(word_ngram).intersection(list(self.ngram_index[ngram])))) / len((set(word_ngram).union((list(self.ngram_index[ngram]))))), 2)\n",
    "        return sorted(self.ngram_index, key=self.ngram_index.get, reverse=True)[:k]\n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)\n",
    "        \n",
    "        \n",
    "class TrigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_trigrams)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.unigram_count['money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nt', 'want', 'dont', 'went', 'moment']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab_word = 'luve'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'lookin', 'took', 'cool', 'hook']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_spelling_corrector = TrigramSpellingCorrector(tf_idf.unigram_count)\n",
    "trigram_spelling_corrector.build_index()\n",
    "trigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Language model\n",
    "Calculate the log likelihood of a sentence. Once with a bigram markovian langauge model, and once with a trigram model.\n",
    "for example - the likelihood of the senetence \"spiderman spiderman does whatever a spider can\" for the bigram model is: \n",
    "$$p(spiderman)\\cdot p(spiderman|spiderman) \\cdot  p(does|spiderman) \\cdot p(whatever|does) \\cdot  p(a|whatever) \\cdot  p(spider|a) \\cdot p(can|spider)$$\n",
    "\n",
    "And for the trigram model:\n",
    "$$p(spiderman,spiderman)\\cdot p(does|spiderman,spiderman) \\cdot  p(whatever|spiderman,does) \\cdot p(a|does,whatever) \\cdot  p(spider|whatever,a) \\cdot  p(can|a, spider)$$\n",
    "\n",
    "Since we do not want a zero probability sentence use Laplace smoothing, as you have seen in the lecture, or here https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log likelihood is -85.36258410538174\n",
      "Trigram log likelihood is -51.891917608100655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-85.36258410538174, -51.891917608100655)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# for the probability smoothing\n",
    "NUMERATOR_SMOOTHING = 1 # alpha in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "DENOMINATOR_SMOOTHING = 10**4 # d in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "def sentence_log_probabilty(unigrams : Counter, bigrams  : Counter,trigrams : Counter, sentence: str):\n",
    "    bigram_log_likelilhood, trigram_log_likelilhood = 0, 0\n",
    "    words_in_sentence = sentence.split()\n",
    "    n_words = len(words_in_sentence)\n",
    "    all_unigrams_count = sum(unigrams.values())\n",
    "    all_bigrams_count = sum(bigrams.values())\n",
    "    all_trigrams_count = sum(trigrams.values())\n",
    "    words_dict = []\n",
    "    p_w_bi = []\n",
    "    p_w_tri = []\n",
    "    alpha= 1\n",
    "    d = 10**4\n",
    "    \n",
    "    for index, word in  enumerate(words_in_sentence):\n",
    "        ### YOUR CODE HERE\n",
    "        words_dict.append(word)\n",
    "        \n",
    "        if index == 0:\n",
    "            p_w_bi.append((unigrams[words_dict[0]] + alpha) /(all_unigrams_count + d))\n",
    "        if index == 1:\n",
    "            temp_bi = (bigrams[words_dict[(index-1)] + ' ' + words_dict[index]] + alpha)/(all_bigrams_count + d) # to delete\n",
    "            p_w_tri.append(temp_bi)\n",
    "            p_w_bi.append((temp_bi/(unigrams[words_dict[index-1]] + alpha)/(all_unigrams_count + d)))\n",
    "        if (index > 1):\n",
    "            temp_bi = (bigrams[words_dict[(index-1)] + ' ' + words_dict[index]] + alpha)/(all_bigrams_count + d)\n",
    "            p_w_bi.append((temp_bi/(unigrams[words_dict[index-1]] + alpha)/(all_unigrams_count + d)))\n",
    "            temp_tri = (trigrams[words_dict[(index-2)] + ' ' + words_dict[(index-1)] + ' ' + words_dict[index]] + alpha)/(all_trigrams_count + d)\n",
    "            p_w_tri.append((temp_tri /(bigrams[words_dict[index-2] + ' ' + words_dict[index-1]] + alpha)/(all_bigrams_count + d)))\n",
    "\n",
    "            \n",
    "\n",
    "    bigram_log_likelilhood = sum([np.log(x) for x in p_w_bi])\n",
    "    trigram_log_likelilhood = sum([np.log(x) for x in p_w_tri])\n",
    "    \n",
    "    print(F\"Bigram log likelihood is {bigram_log_likelilhood}\")\n",
    "    print(F\"Trigram log likelihood is {trigram_log_likelilhood}\")\n",
    "    return bigram_log_likelilhood, trigram_log_likelilhood\n",
    "\n",
    "    \n",
    "sentence = \"green red red \"\n",
    "sentence_log_probabilty(tf_idf.unigram_count, tf_idf.bigram_count, tf_idf.trigram_count, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.51 Language model: B\n",
    "For each model what is the next word prediciton for the sentnence \"i am\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log likelihood is -81.86358185235466\n",
      "Trigram log likelihood is -47.65171403741526\n",
      "Bigram log likelihood is -81.90408883663449\n",
      "Trigram log likelihood is -47.89287609423214\n",
      "Bigram log likelihood is -81.27041864271297\n",
      "Trigram log likelihood is -47.155277151101366\n",
      "Bigram log likelihood is -82.57645604870268\n",
      "Trigram log likelihood is -47.09618823473136\n",
      "Bigram log likelihood is -84.95651645386502\n",
      "Trigram log likelihood is -50.513914918344724\n",
      "Bigram log likelihood is -84.63806272274648\n",
      "Trigram log likelihood is -49.82076773778478\n",
      "Bigram log likelihood is -83.17172565395305\n",
      "Trigram log likelihood is -47.805864717242514\n",
      "Bigram log likelihood is -85.20783088214593\n",
      "Trigram log likelihood is -49.12762055722484\n",
      "Bigram log likelihood is -84.569743478769\n",
      "Trigram log likelihood is -51.90020927946462\n",
      "Bigram log likelihood is -83.36955139728298\n",
      "Trigram log likelihood is -48.855686841741196\n",
      "Bigram log likelihood is -82.7827121507369\n",
      "Trigram log likelihood is -46.20647714066192\n",
      "Bigram log likelihood is -82.22016678324144\n",
      "Trigram log likelihood is -46.776245300061355\n",
      "Bigram log likelihood is -82.32642779423253\n",
      "Trigram log likelihood is -47.33586108799678\n",
      "Bigram log likelihood is -87.62374466044697\n",
      "Trigram log likelihood is -51.20706209890467\n",
      "Bigram log likelihood is -83.5564287706128\n",
      "Trigram log likelihood is -49.12762055722484\n",
      "Bigram log likelihood is -84.72232306636423\n",
      "Trigram log likelihood is -49.502314006666246\n",
      "Bigram log likelihood is -82.01427286526201\n",
      "Trigram log likelihood is -47.305089429330025\n",
      "Bigram log likelihood is -80.90981687844499\n",
      "Trigram log likelihood is -46.712823473623864\n",
      "Bigram log likelihood is -87.28727242382575\n",
      "Trigram log likelihood is -51.90020927946462\n",
      "Bigram log likelihood is -82.67924916885526\n",
      "Trigram log likelihood is -49.0669959354084\n",
      "\n",
      " bigram model prediction is ['nt', 'know', 'oh', 'babi', 'get']\n",
      "\n",
      " trigram model prediction is ['home', 'nt', 'ca', 'gon', 'know']\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "def perdicted_word(unigrams : Counter, bigrams  : Counter,trigrams : Counter, sentence: str):\n",
    "    res_bi = {}\n",
    "    res_tri = {}\n",
    "    words = sentence.split()\n",
    "    words_len = len(words)\n",
    "    if words_len < 2:\n",
    "        return ('sentence is to short for trigrams model')\n",
    "    for i in temp:\n",
    "        bi_gram = words[-1] + ' ' + i\n",
    "        tri_gram = words[-2] + ' ' + words[-1] + ' ' + i\n",
    "        \n",
    "        if bigrams[bi_gram] > 0:\n",
    "            sentence_temp= sentence + ' ' + i\n",
    "            bigram_log,trigram_log = sentence_log_probabilty(unigrams, bigrams,trigrams, sentence_temp)\n",
    "            res_bi[i] = bigram_log\n",
    "            \n",
    "            if trigrams[tri_gram] > 0:\n",
    "                res_tri[i] = trigram_log\n",
    "    \n",
    "    if len(res_bi) == 0:\n",
    "        print('\\n no word match for bigram method')\n",
    "    \n",
    "    else:\n",
    "        print('\\n bigram model prediction is {}'.format(sorted(res_bi, key=res_bi.get, reverse=True)[0:5]))\n",
    "        \n",
    "    if len(res_tri) == 0:\n",
    "        print('\\n no word match for trigram method')\n",
    "    \n",
    "    else:\n",
    "        print('\\n trigram model prediction is {}'.format(sorted(res_tri, key=res_tri.get, reverse=True)[0:5]))\n",
    "    \n",
    "\n",
    "### END YOUR CODE\n",
    "perdicted_word(tf_idf.unigram_count, tf_idf.bigram_count, tf_idf.trigram_count, 'feel like')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
